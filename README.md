# ğŸ  Sherlock Homes - Premium Medellin Real Estate Finder

> [!WARNING]
> **Legal Disclaimer**: This software is for educational and research purposes only. Use it responsibly and at your own risk. See [Legal Disclaimer](legal_disclaimer.md) for full terms.

A powerful web scraping and property aggregation platform for finding rental apartments in MedellÃ­n, Colombia. This application automatically scrapes multiple real estate websites, aggregates listings, and presents them in a beautiful, filterable interface.

![Sherlock Homes](./frontend/assets/images/sherlock_homes.png)

## âœ¨ Features

-  **Automated Web Scraping**: Scrapes multiple real estate websites simultaneously
-  **SQLite Database**: Stores and deduplicates property listings.
-  **Filtering and Sorting**: Filter and sort by multiple options.
-  **Visualization Map**: Displays properties exact location on a map.

## ğŸ¯ Supported Websites

1. **Arrendamientos Envigado** 
2. **Alberto Ãlvarez**
3. **Inmobiliaria Proteger**
4. **Arrendamientos Las Vegas**

## ğŸ“‹ Prerequisites

- Python 3.8 or higher
- pip (Python package manager)
- Internet connection for scraping

## ğŸš€ Installation

### 1. Clone or Download the Repository

```bash
cd /path/to/medellin_real_estate
```

### 2. Create a Virtual Environment (Recommended)

```bash
# Create virtual environment
python3 -m venv backend/venv

# Activate virtual environment
# On macOS/Linux:
source backend/venv/bin/activate

# On Windows:
# backend\venv\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install -r backend/requirements.txt
```

### 4. Install Playwright Browsers

Playwright requires browser binaries to be installed:

```bash
playwright install chromium
```

### 5. Setup Pre-commit Hooks

To ensure code quality and run tests automatically before committing:

```bash
pre-commit install
```

## ğŸ® Usage

### Starting the Application

#### Option 1: Using Uvicorn Directly

```bash
# Make sure you're in the project root directory
cd /Users/simon/.gemini/antigravity/scratch/medellin_real_estate

# Start the server
python3 -m uvicorn backend.main:app --reload --host 0.0.0.0 --port 8000
```

#### Option 2: Using the Run Script

```bash
python3 run_scraper.py
```

### Accessing the Application

Once the server is running, open your browser and navigate to:

```
http://localhost:8000
```

## ğŸ”§ How to Use the Interface

### 1. **Initial Load**
- The app will display any previously scraped properties from the database
- If this is your first time, the database will be empty

### 2. **Scraping Properties**

**Select a Source:**
- Choose from the dropdown: "Arrendamientos Envigado", "Alberto Ãlvarez", or "Todos" (All)

**Force Update (Optional):**
- Check "Forzar actualizaciÃ³n" to bypass the 2-hour cache
- Leave unchecked to use cached data if available

**Click "Buscar Propiedades":**
- The scraper will start collecting properties
- A notification will show progress
- Properties will appear in the grid as they're found

### 3. **Filtering Properties**

Use the filter panel on the left to narrow down results:

- **Precio MÃ­nimo/MÃ¡ximo**: Set price range (in COP)
- **Ãrea MÃ­nima/MÃ¡xima**: Filter by square meters
- **Habitaciones**: Minimum/maximum bedrooms
- **Parqueaderos**: Minimum parking spaces

Filters apply in real-time as you type!

### 4. **Sorting Results**

Use the sort dropdown to organize properties:
- MÃ¡s Recientes (Most Recent)
- Precio: Menor a Mayor (Price: Low to High)
- Precio: Mayor a Menor (Price: High to Low)
- Ãrea: Menor a Mayor (Area: Small to Large)
- Ãrea: Mayor a Menor (Area: Large to Small)

### 5. **Viewing Property Details**

Click "Ver Propiedad" on any card to open the original listing in a new tab.

## ğŸ—‚ï¸ Project Structure

```
medellin_real_estate/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ alberto_alvarez.py      # Alberto Ãlvarez scraper
â”‚   â”‚   â””â”€â”€ arrendamientos_envigado.py  # Arrendamientos Envigado scraper
â”‚   â”œâ”€â”€ database.py                  # SQLAlchemy models and DB setup
â”‚   â”œâ”€â”€ main.py                      # FastAPI application
â”‚   â”œâ”€â”€ scraper.py                   # Scraper orchestration
â”‚   â””â”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html                   # Main UI
â”‚   â”œâ”€â”€ styles.css                   # Styling
â”‚   â””â”€â”€ script.js                    # Frontend logic
â”œâ”€â”€ real_estate.db                   # SQLite database (auto-created)
â”œâ”€â”€ run_scraper.py                   # Convenience script
â””â”€â”€ README.md                        # This file
```

## ğŸ› ï¸ Configuration

### Modifying Search Parameters

#### Price Ranges (Arrendamientos Envigado)

Edit `backend/scrapers/arrendamientos_envigado.py`:

```python
PRICE_RANGES = [
    {"min": 2500000, "max": 3500000}  # Add more ranges as needed
]
```

#### Bedrooms Filter (Alberto Ãlvarez)

Edit `backend/scrapers/alberto_alvarez.py`:

```python
SEARCH_URLS = [
    "https://albertoalvarez.com/inmuebles/arrendamientos/apartamento/envigado/?roomsFrom=1&roomsTo=2"
]
```

Change `roomsFrom` and `roomsTo` parameters as needed.

### Adding New Neighborhoods

Edit the `BARRIOS` dictionary in either scraper file:

```python
BARRIOS = {
    "New Neighborhood": "url-slug-or-id",
    # ... existing neighborhoods
}
```

## ğŸ› Troubleshooting

### "Command not found: uvicorn"

Make sure you've activated the virtual environment:

```bash
source backend/venv/bin/activate
```

Then install dependencies:

```bash
pip install -r backend/requirements.txt
```

### Scraper Not Finding Properties

1. Check your internet connection
2. Verify the target websites are accessible
3. The websites may have changed their HTML structure (requires scraper updates)
4. Try using "Forzar actualizaciÃ³n" to bypass cache

### Database Issues

If you encounter database errors, you can reset it:

```bash
rm real_estate.db
# Restart the application - it will create a fresh database
```

### Playwright Browser Issues

Reinstall Playwright browsers:

```bash
playwright install --force chromium
```

## ğŸ§ª Running Tests

### Backend Tests (Python)
Run the backend tests using `pytest`:

```bash
# Ensure virtual environment is active
# source backend/venv/bin/activate
pytest -n auto  # Run tests in parallel
```

### Frontend Tests (JavaScript)
Run the frontend unit tests using `npm` and `vitest`:

```bash
cd frontend
npm install  # Install dependencies (only needed once)
npm test     # Run tests
```

## ğŸ”’ Rate Limiting & Ethics

- The app includes a 2-hour cooldown between scrapes to be respectful to target websites
- Scraping is done with reasonable delays between requests
- Always respect robots.txt and terms of service of target websites
- This tool is for personal use and research purposes

## ğŸ“Š Database Schema

The SQLite database stores properties with the following fields:

- `id`: Primary key
- `code`: Property reference code
- `title`: Property title
- `location`: Neighborhood/location
- `price`: Monthly rent price
- `area`: Square meters
- `bedrooms`: Number of bedrooms
- `bathrooms`: Number of bathrooms
- `parking`: Number of parking spaces
- `estrato`: Socioeconomic stratum
- `image_url`: Main property image
- `link`: URL to original listing
- `source`: Website source
- `created_at`: Timestamp when added

## ğŸš€ Advanced Usage

### Running Scrapers Programmatically

You can import and run scrapers directly:

```python
import asyncio
from backend.scrapers import alberto_alvarez, arrendamientos_envigado

# Run a specific scraper
async def main():
    results = await alberto_alvarez.scrape()
    print(f"Found {len(results)} properties")

asyncio.run(main())
```

### API Endpoints

The FastAPI backend exposes these endpoints:

- `GET /` - Serves the frontend
- `GET /api/properties?skip=0&limit=100` - Get properties from database
- `POST /api/scrape/batch?source=all&force=false` - Trigger batch scraping

## ğŸ“ License

This project is licensed under the [MIT License](LICENSE).

See the [Legal Disclaimer](legal_disclaimer.md) for important information regarding the use of this software, data ownership, and liability.

## ğŸ¤ Contributing

This is a personal project, but suggestions and improvements are welcome!

## ğŸ“§ Support

For issues or questions, please check the troubleshooting section above.

---

**Happy House Hunting! ğŸ¡**
